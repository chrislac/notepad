<p>Kubernetes Clusters had it’s own internal network. This network is completely segregated from your local network.</p>
<p>While deploying your kubernetes cluster (doesn’t matter if it’s rancher or any other on-premises kubernetes cluster) you can define on which CIDR your cluster will sit.</p>
<p>So if kubernetes has it’s own network, how can I talk to the applications I deployed in my cluster?</p>
<p>You can expose your resources by using a Service or a Ingress. For example: When you create a service with <code>type: LoadBalancer</code> your service will allocate a external or public IP address (endpoint)  that can be acceded from your internal network.</p>
<pre><code>$ kubectl get svc
NAME                                      TYPE           CLUSTER-IP   EXTERNAL-IP      PORT(S)                      AGE
custom-nginx-svc                          LoadBalancer   10.0.10.18   104.155.87.232   80:31549/TCP                 11d
echo-svc                                  LoadBalancer   10.0.10.14   23.251.138.185   80:30668/TCP                 11d
kubernetes                                ClusterIP      10.0.0.1     &lt;none&gt;           443/TCP                      11d
nginx-ing-nginx-ingress-controller        NodePort       10.0.9.184   &lt;none&gt;           80:31745/TCP,443:31748/TCP   25h
nginx-ing-nginx-ingress-default-backend   ClusterIP      10.0.1.169   &lt;none&gt;           80/TCP                       25h
</code></pre>
<p>As can be seen in the example above, there are two services with external IP defined.</p>
<p>In your scenario you need these External IPs to be IPs from your local network. This can be achieved using MetalLB.</p>

