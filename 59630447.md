<p>As @D.T. explained a persistent volume claim is exclusively bound to a persistent volume.<br>
<strong>You cannot bind 2 pvc to the same pv</strong>.</p>
<p><a href="https://stackoverflow.com/questions/44204223/kubernetes-nfs-persistent-volumes-multiple-claims-on-same-volume-claim-stuck">Here</a> you can find another case where it was discussed.</p>
<p>There is a better solution for your scenario and it involves using  <a href="https://github.com/helm/charts/tree/master/stable/nfs-client-provisioner">nfs-client-provisioner</a>. To achive that, firstly you have to install helm in your cluster an than follow these steps that I created for a previous <a href="https://serverfault.com/questions/990520/nfs-server-and-client-are-working-but-data-is-not-on-the-server/990621#990621">answer</a> on ServerFault.</p>
<p>I’ve tested it and using this solution you can isolate one PVC from the other.</p>
<p><strong>1 - Install and configur NFS Server on my Master Node (Debian Linux, this might change depending on your Linux distribution):</strong></p>
<p>Before installing the NFS Kernel server, we need to update our system’s repository index:</p>
<pre><code>$ sudo apt-get update
</code></pre>
<p>Now, run the following command in order to install the NFS Kernel Server on your system:</p>
<pre><code>$ sudo apt install nfs-kernel-server
</code></pre>
<p>Create the Export Directory</p>
<pre><code>$ sudo mkdir -p /mnt/nfs_server_files
</code></pre>
<p>As we want all clients to access the directory, we will remove restrictive permissions of the export folder through the following commands (this may vary on your set-up according to your security policy):</p>
<pre><code>$ sudo chown nobody:nogroup /mnt/nfs_server_files
$ sudo chmod 777 /mnt/nfs_server_files
</code></pre>
<p>Assign server access to client(s) through NFS export file</p>
<pre><code>$ sudo nano /etc/exports
</code></pre>
<p>Inside this file, add a new line to allow access from other servers to your share.</p>
<pre><code>/mnt/nfs_server_files        10.128.0.0/24(rw,sync,no_subtree_check)
</code></pre>
<p>You may want to use different options in your share. 10.128.0.0/24 is my k8s internal network.</p>
<p>Export the shared directory and restart the service to make sure all configuration files are correct.</p>
<pre><code>$ sudo exportfs -a
$ sudo systemctl restart nfs-kernel-server
</code></pre>
<p>Check all active shares:</p>
<pre><code>$ sudo exportfs
/mnt/nfs_server_files
                10.128.0.0/24
</code></pre>
<p><strong>2 - Install NFS Client on all my Worker Nodes:</strong></p>
<pre><code>$ sudo apt-get update
$ sudo apt-get install nfs-common
</code></pre>
<p>At this point you can make a test to check if you have access to your share from your worker nodes:</p>
<pre><code>$ sudo mkdir -p /mnt/sharedfolder_client
$ sudo mount kubemaster:/mnt/nfs_server_files /mnt/sharedfolder_client
</code></pre>
<p>Notice that at this point you can use the name of your master node. K8s is taking care of the DNS here.<br>
Check if the volume mounted as expected and create some folders and files to male sure everything is working fine.</p>
<pre><code>$ cd /mnt/sharedfolder_client
$ mkdir test
$ touch file
</code></pre>
<p>Go back to your master node and check if these files are at /mnt/nfs_server_files folder.</p>
<p><strong>3 - Install NFS Client Provisioner</strong>.</p>
<p>Install the provisioner using helm:</p>
<pre><code>$ helm install --name ext --namespace nfs --set nfs.server=kubemaster --set nfs.path=/mnt/nfs_server_files stable/nfs-client-provisioner
</code></pre>
<p>Notice that I’ve specified a namespace for it.<br>
Check if they are running:</p>
<pre><code>$ kubectl get pods -n nfs
NAME                                         READY   STATUS      RESTARTS   AGE
ext-nfs-client-provisioner-f8964b44c-2876n   1/1     Running     0          84s
</code></pre>
<p>At this point we have a storageclass called nfs-client:</p>
<pre><code>$ kubectl get storageclass -n nfs
NAME         PROVISIONER                                AGE
nfs-client   cluster.local/ext-nfs-client-provisioner   5m30s
</code></pre>
<p>We need to create a PersistentVolumeClaim:</p>
<pre><code>$ more nfs-client-pvc.yaml

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  namespace: nfs 
  name: test-claim
  annotations:
    volume.beta.kubernetes.io/storage-class: "nfs-client"
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi
</code></pre>
<pre><code>$ kubectl apply -f nfs-client-pvc.yaml
</code></pre>
<p>Check the status (Bound is expected):</p>
<pre><code>$ kubectl get persistentvolumeclaim/test-claim -n nfs
NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
test-claim   Bound    pvc-e1cd4c78-7c7c-4280-b1e0-41c0473652d5   1Mi        RWX            nfs-client     24s
</code></pre>
<p><strong>4 - Create a simple pod to test if we can read/write out NFS Share:</strong></p>
<p>Create a pod using this yaml:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: pod0
  labels:
    env: test
  namespace: nfs  
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
    volumeMounts:
      - name: nfs-pvc
        mountPath: "/mnt"
  volumes:
    - name: nfs-pvc
      persistentVolumeClaim:
        claimName: test-claim
</code></pre>
<pre><code>$ kubectl apply -f pod.yaml
</code></pre>
<p>Let’s list all mounted volumes on our pod:</p>
<pre><code>$ kubectl exec -ti -n nfs pod0 -- df -h /mnt
Filesystem                                                                               Size  Used Avail Use% Mounted on
kubemaster:/mnt/nfs_server_files/nfs-test-claim-pvc-a2e53b0e-f9bb-4723-ad62-860030fb93b1   99G   11G   84G  11% /mnt
</code></pre>
<p>As we can see, we have a NFS volume mounted on /mnt. (Important to notice the path <code>kubemaster:/mnt/nfs_server_files/nfs-test-claim-pvc-a2e53b0e-f9bb-4723-ad62-860030fb93b1</code>)</p>
<p>Let’s check it:</p>
<pre><code>root@pod0:/# cd /mnt
root@pod0:/mnt# ls -la
total 8
drwxrwxrwx 2 nobody nogroup 4096 Nov  5 08:33 .
drwxr-xr-x 1 root   root    4096 Nov  5 08:38 ..
</code></pre>
<p>It’s empty. Let’s create some files:</p>
<pre><code>$ for i in 1 2; do touch file$i; done;
$ ls -l 
total 8
drwxrwxrwx 2 nobody nogroup 4096 Nov  5 08:58 .
drwxr-xr-x 1 root   root    4096 Nov  5 08:38 ..
-rw-r--r-- 1 nobody nogroup    0 Nov  5 08:58 file1
-rw-r--r-- 1 nobody nogroup    0 Nov  5 08:58 file2
</code></pre>
<p>Now let’s where are these files on our NFS Server (Master Node):</p>
<pre><code>$ cd /mnt/nfs_server_files
$ ls -l 
total 4
drwxrwxrwx 2 nobody nogroup 4096 Nov  5 09:11 nfs-test-claim-pvc-4550f9f0-694d-46c9-9e4c-7172a3a64b12
$ cd nfs-test-claim-pvc-4550f9f0-694d-46c9-9e4c-7172a3a64b12/
$ ls -l 
total 0
-rw-r--r-- 1 nobody nogroup 0 Nov  5 09:11 file1
-rw-r--r-- 1 nobody nogroup 0 Nov  5 09:11 file2
</code></pre>
<p>And here are the files we just created inside our pod!</p>

